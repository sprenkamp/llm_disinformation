{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, user, assistant, select, system\n",
    "import os   \n",
    "\n",
    "# load a model (could be Transformers, LlamaCpp, VertexAI, OpenAI...)\n",
    "gpt = models.OpenAI(\"gpt-3.5-turbo\", api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with system():\n",
    "    lm = gpt + \"\"\"Please classify the given text into one of the following categories:\n",
    "    true:The statement is accurate and there is nothing significant missing.\n",
    "    mostly-true:The statement is accurate but needs clarification or additional information.\n",
    "    Half True:The statement is partially accurate but leaves out important details or takes things out of context.\n",
    "    barely-true:The statement contains an element of truth but ignores critical facts that would give a different impression.\n",
    "    false:The statement is not accurate.\n",
    "    pants-fire:The statement is not accurate and makes a ridiculous claim including a conspiracy.\n",
    "    The statement will have the following format:\n",
    "    statement|author|date\n",
    "    e.g.:'Last year alone, natural disasters in America caused $178 billion in damages.|Barack Obama|2013-02-13'\"\"\" #NOTE the example is fictional\n",
    "with user():\n",
    "    #lm += \"Last year alone, natural disasters in America caused $178 billion in damages.\"\n",
    "    #lm += \"In every single war that America has fought, we have never asked for land afterwards, except for enough to bury the Americans who gave the ultimate sacrifice for that freedom we went in for.|Kevin McCarthy|2023-11-26\"\n",
    "    #lm += \"Because wages are rising, this Thanksgiving dinner is the fourth-cheapest ever as a percentage of average earnings.|Karine Jean-Pierre|2023-11-20\"\n",
    "    lm += \"Russland bombed the kharshan dam.|Christine Drazen|2022-01-05\"\n",
    "# with assistant():\n",
    "#     lm += select(['True', 'Mostly True', 'Half True', \"Mostly False\", \"False\", \"Pants on Fire\"], name = 'output_classification')\n",
    "\n",
    "with assistant():\n",
    "    lm += \"Here is a reason about my classification: \" + gen(name=\"reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Please tell me a joke:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kiliansprenkamp/Desktop/code/llm_disinformation/src/llm/guidance/test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kiliansprenkamp/Desktop/code/llm_disinformation/src/llm/guidance/test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lm \u001b[39m=\u001b[39m gpt \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mPlease tell me a joke:\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m gen(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjoke\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages/guidance/models/_model.py:242\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m# run stateless functions (grammar nodes)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, StatelessFunction):\n\u001b[0;32m--> 242\u001b[0m     out \u001b[39m=\u001b[39m lm\u001b[39m.\u001b[39;49m_run_stateless(value)\n\u001b[1;32m    244\u001b[0m \u001b[39m# run stateful functions\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     out \u001b[39m=\u001b[39m value(lm)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages/guidance/models/_model.py:382\u001b[0m, in \u001b[0;36mModel._run_stateless\u001b[0;34m(lm, stateless_function, max_tokens, temperature, top_p, n)\u001b[0m\n\u001b[1;32m    380\u001b[0m delayed_bytes \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m \u001b[39m# last_is_generated = False\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[39mfor\u001b[39;49;00m new_bytes, is_generated, new_bytes_log_prob, capture_groups, capture_group_log_probs, new_token_count \u001b[39min\u001b[39;49;00m gen_obj:\n\u001b[1;32m    383\u001b[0m     \u001b[39m# convert the bytes to a string (delaying if we don't yet have a valid unicode string)\u001b[39;49;00m\n\u001b[1;32m    384\u001b[0m     lm\u001b[39m.\u001b[39;49mtoken_count \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m new_token_count\n\u001b[1;32m    385\u001b[0m     new_bytes \u001b[39m=\u001b[39;49m delayed_bytes \u001b[39m+\u001b[39;49m new_bytes\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages/guidance/models/_local.py:257\u001b[0m, in \u001b[0;36mLocal.__call__\u001b[0;34m(self, grammar, max_tokens, n, top_p, temperature, ensure_bos_token, log_probs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     token_ids,token_byte_positions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cleanup_tokens(token_ids, token_byte_positions)\n\u001b[1;32m    256\u001b[0m     was_forced \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_logits(token_ids, parser\u001b[39m.\u001b[39;49mbytes[start_pos:forced_pos])\n\u001b[1;32m    259\u001b[0m \u001b[39m# if requested we compute the log probabilities so we can track the probabilities of each node\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# TODO: we should lower this step to C++ with pybind11\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m log_probs:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages/guidance/models/_remote.py:174\u001b[0m, in \u001b[0;36mRemote._get_logits\u001b[0;34m(self, token_ids, forced_bytes)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_new_stream(prompt)\n\u001b[1;32m    172\u001b[0m     \u001b[39m# we wait for the running stream to put something in the queue\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_state[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shared_state[\u001b[39m\"\u001b[39;49m\u001b[39mdata_queue\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    176\u001b[0m \u001b[39m# # if we don't have the next byte of data yet then we wait for it (from the streaming thread)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m# if len(self._shared_state[\"data\"]) == len(prompt):\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m#     self._shared_state[\"data\"] += self._shared_state[\"data\"]_queue.get() \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[39m# set the logits to the next byte the model picked\u001b[39;00m\n\u001b[1;32m    183\u001b[0m logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens)) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mnan\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_disinformation/lib/python3.12/threading.py:334\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    335\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lm = gpt + \"Please tell me a joke:\" + gen(name=\"joke\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One surprising fact about Paris is that there is only one stop sign in the entire city.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['fact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting guidance\n",
      "  Using cached guidance-0.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting diskcache (from guidance)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting gptcache (from guidance)\n",
      "  Using cached gptcache-0.1.43-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting openai>=1.0 (from guidance)\n",
      "  Using cached openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/kiliansprenkamp/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages (from guidance) (4.0.0)\n",
      "Collecting tiktoken>=0.3 (from guidance)\n",
      "  Using cached tiktoken-0.5.1.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio in /Users/kiliansprenkamp/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages (from guidance) (1.5.8)\n",
      "Collecting msal (from guidance)\n",
      "  Using cached msal-1.25.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting requests (from guidance)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting numpy (from guidance)\n",
      "  Using cached numpy-1.26.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scipy (from guidance)\n",
      "  Using cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl.metadata (217 kB)\n",
      "Collecting aiohttp (from guidance)\n",
      "  Using cached aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting ordered-set (from guidance)\n",
      "  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting pyformlang (from guidance)\n",
      "  Using cached pyformlang-1.0.4-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai>=1.0->guidance)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.0->guidance)\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.0->guidance)\n",
      "  Using cached httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai>=1.0->guidance)\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting sniffio (from openai>=1.0->guidance)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai>=1.0->guidance)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/kiliansprenkamp/miniconda3/envs/llm_disinformation/lib/python3.12/site-packages (from openai>=1.0->guidance) (4.8.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.3->guidance)\n",
      "  Using cached regex-2023.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->guidance)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->guidance)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->guidance)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->guidance)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->guidance)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->guidance)\n",
      "  Using cached multidict-6.0.4-cp312-cp312-macosx_11_0_arm64.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->guidance)\n",
      "  Using cached yarl-1.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->guidance)\n",
      "  Using cached frozenlist-1.4.0-cp312-cp312-macosx_11_0_arm64.whl\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->guidance)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting cachetools (from gptcache->guidance)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal->guidance)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cryptography<44,>=0.6 (from msal->guidance)\n",
      "  Using cached cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl.metadata (5.2 kB)\n",
      "Collecting networkx (from pyformlang->guidance)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pydot (from pyformlang->guidance)\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting cffi>=1.12 (from cryptography<44,>=0.6->msal->guidance)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0->guidance)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->guidance)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai>=1.0->guidance)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai>=1.0->guidance)\n",
      "  Using cached pydantic_core-2.14.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting pyparsing>=2.1.4 (from pydot->pyformlang->guidance)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography<44,>=0.6->msal->guidance)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Using cached guidance-0.1.4-py3-none-any.whl (80 kB)\n",
      "Using cached openai-1.3.6-py3-none-any.whl (220 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl (388 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached gptcache-0.1.43-py3-none-any.whl (131 kB)\n",
      "Using cached msal-1.25.0-py2.py3-none-any.whl (97 kB)\n",
      "Using cached numpy-1.26.2-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached pyformlang-1.0.4-py3-none-any.whl (124 kB)\n",
      "Using cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl (29.6 MB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl (5.3 MB)\n",
      "Using cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.5-cp312-cp312-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached regex-2023.10.3-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached yarl-1.9.3-cp312-cp312-macosx_11_0_arm64.whl (78 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (177 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Building wheels for collected packages: tiktoken\n",
      "  Building wheel for tiktoken (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tiktoken: filename=tiktoken-0.5.1-cp312-cp312-macosx_14_0_arm64.whl size=909733 sha256=5c6b19674587571114399a7f6ee8a10733f0ca9fa4c557f23cdad9c2d64a0292\n",
      "  Stored in directory: /Users/kiliansprenkamp/Library/Caches/pip/wheels/e7/7d/04/bf4aa1dcb54b90d78df91e45cb5f48f7d2ae11fc3486d56162\n",
      "Successfully built tiktoken\n",
      "Installing collected packages: urllib3, tqdm, sniffio, regex, pyparsing, PyJWT, pydantic-core, pycparser, ordered-set, numpy, networkx, multidict, idna, h11, frozenlist, distro, diskcache, charset-normalizer, certifi, cachetools, attrs, annotated-types, yarl, scipy, requests, pydot, pydantic, httpcore, cffi, anyio, aiosignal, tiktoken, pyformlang, httpx, gptcache, cryptography, aiohttp, openai, msal, guidance\n",
      "Successfully installed PyJWT-2.8.0 aiohttp-3.9.1 aiosignal-1.3.1 annotated-types-0.6.0 anyio-3.7.1 attrs-23.1.0 cachetools-5.3.2 certifi-2023.11.17 cffi-1.16.0 charset-normalizer-3.3.2 cryptography-41.0.7 diskcache-5.6.3 distro-1.8.0 frozenlist-1.4.0 gptcache-0.1.43 guidance-0.1.4 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 idna-3.6 msal-1.25.0 multidict-6.0.4 networkx-3.2.1 numpy-1.26.2 openai-1.3.6 ordered-set-4.1.0 pycparser-2.21 pydantic-2.5.2 pydantic-core-2.14.5 pydot-1.4.2 pyformlang-1.0.4 pyparsing-3.1.1 regex-2023.10.3 requests-2.31.0 scipy-1.11.4 sniffio-1.3.0 tiktoken-0.5.1 tqdm-4.66.1 urllib3-2.1.0 yarl-1.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_disinformation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
